{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quantisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bd3995a65b23b58"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:51:28.630737Z",
     "start_time": "2024-06-30T17:51:27.356849Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using MPS\n"
     ]
    },
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "random.seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(\"Using GPU\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    logger.info(\"Using MPS\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    logger.info(\"Using CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22b864d72660c49b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_token(ins: dict, model) -> (torch.Tensor, torch.Tensor):\n",
    "    with torch.no_grad():\n",
    "        _output = model(**ins)\n",
    "\n",
    "    _next_token = _output.logits[:, -1, :].argmax(dim=1)\n",
    "    return _output, _next_token\n",
    "\n",
    "def chat(model, tokeniser, inputs_t0, no_of_tokens = 100):\n",
    "    generated_tokens = dict()\n",
    "    inputs_tx = inputs_t0\n",
    "    if \"position_ids\" in inputs_t0:\n",
    "        position_ids = inputs_t0[\"position_ids\"]\n",
    "    else:\n",
    "        position_ids = None\n",
    "    durations_cached_s = []\n",
    "    for _ in range(no_of_tokens):\n",
    "        t0 = time.time()\n",
    "        output, next_token_ids = generate_token(inputs_tx, model)\n",
    "        durations_cached_s.append(time.time() - t0)\n",
    "        \n",
    "        inputs_tx = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [inputs_tx[\"attention_mask\"], torch.ones((inputs_t0[\"input_ids\"].shape[0], 1))],\n",
    "                dim=1\n",
    "            ),\n",
    "            \"past_key_values\": output.past_key_values,\n",
    "        }\n",
    "        \n",
    "        if position_ids is not None:\n",
    "            position_ids = position_ids[:, -1].unsqueeze(-1) + 1\n",
    "            inputs_tx[\"position_ids\"] = position_ids\n",
    "            \n",
    "        \n",
    "        next_tokens = tokeniser.batch_decode(next_token_ids.reshape((inputs_t0[\"input_ids\"].shape[0], 1)))\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            if i not in generated_tokens:\n",
    "                generated_tokens[i] = []\n",
    "            generated_tokens[i].append(token)\n",
    "            \n",
    "    return [\"\".join(generated_tokens[i]) for i in generated_tokens.keys()], durations_cached_s\n",
    "\n",
    "# fix dtype post quantization to \"pretend\" to be fp32\n",
    "def get_float32_dtype(self):\n",
    "    return torch.float32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:52:51.748510Z",
     "start_time": "2024-06-30T17:52:51.744560Z"
    }
   },
   "id": "f5427182c7d3d46e",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f9e7fe27658c7e6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokeniser = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", )\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "gpt_tokeniser.pad_token = gpt_tokeniser.eos_token\n",
    "gpt2.config.pad_token_id = gpt2.config.eos_token_id\n",
    "\n",
    "# pad on the left so we can append new tokens on the right\n",
    "gpt_tokeniser.padding_side = \"left\"\n",
    "gpt_tokeniser.truncation_side = \"left\"\n",
    "\n",
    "gpt2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:52:54.161138Z",
     "start_time": "2024-06-30T17:52:53.390316Z"
    }
   },
   "id": "8ff26ffa3a1b93f6",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: GPT2 footprint: 486.7 MB\n"
     ]
    }
   ],
   "source": [
    "GPT2Model.dtype = property(get_float32_dtype)\n",
    "logger.info(f\" GPT2 footprint: {round(gpt2.get_memory_footprint() / (1024 * 1024), 2)} MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:55:36.801072Z",
     "start_time": "2024-06-30T17:55:36.797816Z"
    }
   },
   "id": "bc06ebafc2539fec",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scripting Quantisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a2303f6f2fafd0f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def quantise(t: torch.Tensor) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    _min, _max = t.min(), t.max()\n",
    "    scale = (_max - _min) / 255\n",
    "    zero_point = _min\n",
    "    t = (t - zero_point) / scale\n",
    "    t = torch.clamp(t, min=0, max=255)\n",
    "    t = t.type(torch.uint8)\n",
    "    return t, (scale, zero_point)\n",
    "\n",
    "def dequantise(t: torch.Tensor, state: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "    scale, zero_point = state\n",
    "    return t.to(torch.float32) * scale + zero_point\n",
    "\n",
    "def quantise_model(model: torch.nn.Module) -> Tuple[torch.nn.Module, dict]:\n",
    "    states = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "        param.data, state = quantise(param.data)\n",
    "        states[name] = state\n",
    "    return model, states\n",
    "\n",
    "def dequantise_model(model: torch.nn.Module, states: dict) -> torch.nn.Module:\n",
    "    for name, param in model.named_parameters():\n",
    "        param.data = dequantise(param.data, states[name])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T20:15:56.919186Z",
     "start_time": "2024-06-30T20:15:56.913541Z"
    }
   },
   "id": "579501479ab2437a",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: Min and Max before quantisation: -2.8436343669891357, 2.7956299781799316, torch.float32\n",
      "INFO:__main__: Min and Max after quantisation: 0, 255, torch.uint8\n",
      "INFO:__main__: Min and Max after dequantisation: -2.8436343669891357, 2.7956297397613525, torch.float32\n",
      "INFO:__main__: Total loss: 19566.611328125\n"
     ]
    }
   ],
   "source": [
    "_t = gpt2.transformer.h[0].attn.c_attn.weight.data\n",
    "logger.info(f\" Min and Max before quantisation: {_t.min().item()}, {_t.max().item()}, {_t.dtype}\")\n",
    "\n",
    "_tq, _state = quantise(_t)\n",
    "logger.info(f\" Min and Max after quantisation: {_tq.min().item()}, {_tq.max().item()}, {_tq.dtype}\")\n",
    "\n",
    "_tc = dequantise(_tq, _state)\n",
    "logger.info(f\" Min and Max after dequantisation: {_tc.min().item()}, {_tc.max().item()}, {_tc.dtype}\")\n",
    "\n",
    "_total_loss = torch.abs(_t - _tc).sum()\n",
    "logger.info(f\" Total loss: {_total_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T20:06:30.216896Z",
     "start_time": "2024-06-30T20:06:30.206099Z"
    }
   },
   "id": "14c39ad17023d830",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Applying Quantisation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "763bd94e71b66421"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Before"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "685f97165fcb35e7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: GPT2 footprint: 486.7 MB\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\" GPT2 footprint: {round(gpt2.get_memory_footprint() / (1024 * 1024), 2)} MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T20:19:04.397332Z",
     "start_time": "2024-06-30T20:19:04.394919Z"
    }
   },
   "id": "ecefcc3127701639",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[' I saw a man standing in the middle of the street. He was wearing a black hoodie and']"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_t0 = gpt_tokeniser(\"I woke up to the rain and as I looked outside the window\", return_tensors=\"pt\")\n",
    "expected_response, _ = chat(gpt2, gpt_tokeniser, inputs_t0, 20)\n",
    "expected_response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T20:29:26.235030Z",
     "start_time": "2024-06-30T20:29:25.767561Z"
    }
   },
   "id": "c666856d91121753",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d241fcb4241b73b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:After quantisation: 130.6750946044922 MB\n",
      "INFO:__main__:After dequantisation: 486.7002410888672 MB\n"
     ]
    }
   ],
   "source": [
    "gpt2q, _states = quantise_model(copy.deepcopy(gpt2))\n",
    "logger.info(f\"After quantisation: {gpt2q.get_memory_footprint() / (1024 * 1024)} MB\")\n",
    "\n",
    "gpt2q = dequantise_model(gpt2q, _states)\n",
    "logger.info(f\"After dequantisation: {gpt2q.get_memory_footprint() / (1024 * 1024)} MB\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T20:30:17.754308Z",
     "start_time": "2024-06-30T20:30:17.405351Z"
    }
   },
   "id": "770c1bf134222ee5",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Results:\n",
      "Expected Result: I saw a man standing in the middle of the street. He was wearing a black hoodie and\n",
      "Quantised Result: I saw a man standing there with his head down on the ground. He was wearing a shirt and\n"
     ]
    }
   ],
   "source": [
    "quantised_response, _ = chat(gpt2q, gpt_tokeniser, inputs_t0, 20)\n",
    "logger.info(f\"Results:\\nExpected Result:{expected_response[0]}\\nQuantised Result:{quantised_response[0]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T20:32:16.598667Z",
     "start_time": "2024-06-30T20:32:16.119615Z"
    }
   },
   "id": "8b1e60530b0eddcb",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "825f37ea172b7c6f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
